import os
from openai import OpenAI
from dotenv import load_dotenv
import requests
from xml.etree import ElementTree as ET
import json
from datetime import datetime
from typing import List, Dict, Optional, Callable, Any
import time
import asyncio
import aiohttp
from concurrent.futures import ThreadPoolExecutor, as_completed
from dataclasses import dataclass
from enum import Enum
import sys

load_dotenv()

client = OpenAI(
    api_key=os.environ.get('DEEPSEEK_API_KEY'),
    base_url="https://api.deepseek.com"
)


# ========== å·¥å…·é“¾è®¾è®¡ ==========

class ToolPriority(Enum):
    """å·¥å…·ä¼˜å…ˆçº§"""
    HIGH = 1      # å¿…é¡»ç«‹å³æ‰§è¡Œ
    MEDIUM = 2    # å¯ä»¥å¹¶è¡Œæ‰§è¡Œ
    LOW = 3       # å¯ä»¥å»¶è¿Ÿæ‰§è¡Œ


@dataclass
class ToolDependency:
    """å·¥å…·ä¾èµ–å…³ç³»"""
    name: str
    depends_on: List[str] = None  # ä¾èµ–çš„å·¥å…·
    can_parallel: bool = True      # æ˜¯å¦å¯ä»¥å¹¶è¡Œ
    priority: ToolPriority = ToolPriority.MEDIUM
    timeout: int = 30              # è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰


class ToolChain:
    """å·¥å…·é“¾ç®¡ç†å™¨"""

    def __init__(self):
        # å®šä¹‰å·¥å…·ä¾èµ–å…³ç³»
        self.dependencies = {
            "search_pubmed": ToolDependency(
                name="search_pubmed",
                depends_on=None,  # æ— ä¾èµ–ï¼Œå¯ä»¥ç«‹å³æ‰§è¡Œ
                can_parallel=True,
                priority=ToolPriority.HIGH
            ),
            "fetch_pubmed_details": ToolDependency(
                name="fetch_pubmed_details",
                depends_on=["search_pubmed"],  # ä¾èµ–æœç´¢ç»“æœ
                can_parallel=False,
                priority=ToolPriority.MEDIUM,
                timeout=45
            ),
            "fetch_full_text_links": ToolDependency(
                name="fetch_full_text_links",
                depends_on=["search_pubmed"],
                can_parallel=True,  # å¯ä»¥å¹¶è¡Œè·å–å¤šç¯‡æ–‡ç« çš„é“¾æ¥
                priority=ToolPriority.LOW,
                timeout=20
            )
        }

    def can_execute_parallel(self, tool_calls: List[Dict]) -> bool:
        """åˆ¤æ–­å·¥å…·è°ƒç”¨æ˜¯å¦å¯ä»¥å¹¶è¡Œæ‰§è¡Œ"""
        if len(tool_calls) <= 1:
            return False

        # æ£€æŸ¥æ‰€æœ‰å·¥å…·æ˜¯å¦éƒ½æ”¯æŒå¹¶è¡Œ
        for call in tool_calls:
            tool_name = call['function']['name']
            if tool_name in self.dependencies:
                if not self.dependencies[tool_name].can_parallel:
                    return False

        return True

    def group_by_dependency(self, tool_calls: List[Dict]) -> List[List[Dict]]:
        """æŒ‰ä¾èµ–å…³ç³»åˆ†ç»„å·¥å…·è°ƒç”¨"""
        groups = []
        independent = []  # æ— ä¾èµ–çš„å·¥å…·
        dependent = []    # æœ‰ä¾èµ–çš„å·¥å…·

        for call in tool_calls:
            tool_name = call['function']['name']
            dep = self.dependencies.get(tool_name)

            if not dep or not dep.depends_on:
                independent.append(call)
            else:
                dependent.append(call)

        if independent:
            groups.append(independent)
        if dependent:
            groups.append(dependent)

        return groups


# ========== å¼‚æ­¥ HTTP è¯·æ±‚ï¼ˆæ”¯æŒå¹¶è¡Œï¼‰==========

class AsyncPubMedClient:
    """å¼‚æ­¥ PubMed å®¢æˆ·ç«¯"""

    def __init__(self):
        self.base_url = "https://eutils.ncbi.nlm.nih.gov/entrez/eutils"
        self.session = None

    async def __aenter__(self):
        self.session = aiohttp.ClientSession()
        return self

    async def __aexit__(self, exc_type, exc_val, exc_tb):
        if self.session:
            await self.session.close()

    async def search_pubmed(self, query: str, max_results: int = 10, sort: str = "relevance") -> Dict:
        """å¼‚æ­¥æœç´¢"""
        url = f"{self.base_url}/esearch.fcgi"
        params = {
            "db": "pubmed",
            "term": query,
            "retmax": max_results,
            "retmode": "json",
            "sort": sort
        }

        try:
            async with self.session.get(url, params=params, timeout=10) as response:
                response.raise_for_status()
                data = await response.json()

                pmids = data.get('esearchresult', {}).get('idlist', [])
                total_count = int(data.get('esearchresult', {}).get('count', 0))

                return {
                    "success": True,
                    "pmids": pmids,
                    "count": len(pmids),
                    "total_available": total_count,
                    "query": query
                }
        except Exception as e:
            return {"success": False, "error": str(e), "query": query}

    async def fetch_details(self, pmids: List[str]) -> Dict:
        """å¼‚æ­¥è·å–è¯¦æƒ…"""
        if not pmids:
            return {"success": False, "error": "æœªæä¾› PMID"}

        pmid_string = ','.join(pmids)
        url = f"{self.base_url}/efetch.fcgi"
        params = {
            "db": "pubmed",
            "id": pmid_string,
            "retmode": "xml"
        }

        try:
            async with self.session.get(url, params=params, timeout=15) as response:
                response.raise_for_status()
                content = await response.read()
                root = ET.fromstring(content)

                articles = []
                for article in root.findall('.//PubmedArticle'):
                    pmid = article.findtext('.//PMID')
                    title = article.findtext('.//ArticleTitle') or "æ— æ ‡é¢˜"

                    # æå–æ‘˜è¦
                    abstract_parts = article.findall('.//AbstractText')
                    abstract_full = ' '.join([part.text or '' for part in abstract_parts])
                    abstract = abstract_full[:800] + "..." if len(abstract_full) > 800 else abstract_full

                    # æå–ä½œè€…
                    authors = []
                    for author in article.findall('.//Author'):
                        last = author.findtext('.//LastName', '')
                        first = author.findtext('.//ForeName', '')
                        if last:
                            authors.append(f"{first} {last}".strip())

                    journal = article.findtext('.//Journal/Title', 'æœªçŸ¥æœŸåˆŠ')
                    year = article.findtext('.//PubDate/Year', 'æœªçŸ¥å¹´ä»½')

                    # DOI
                    doi = None
                    for article_id in article.findall('.//ArticleId'):
                        if article_id.get('IdType') == 'doi':
                            doi = article_id.text

                    articles.append({
                        "pmid": pmid,
                        "title": title,
                        "abstract": abstract,
                        "authors": authors[:5],
                        "authors_display": ", ".join(authors[:3]) + ("ç­‰" if len(authors) > 3 else ""),
                        "journal": journal,
                        "year": year,
                        "doi": doi,
                        "pubmed_url": f"https://pubmed.ncbi.nlm.nih.gov/{pmid}/",
                        "doi_url": f"https://doi.org/{doi}" if doi else None
                    })

                return {
                    "success": True,
                    "articles": articles,
                    "count": len(articles)
                }
        except Exception as e:
            return {"success": False, "error": str(e)}

    async def fetch_full_text_link(self, pmid: str) -> Dict:
        """å¼‚æ­¥è·å–å…¨æ–‡é“¾æ¥ï¼ˆå•ä¸ªï¼‰"""
        url = f"{self.base_url}/elink.fcgi"
        params = {
            "dbfrom": "pubmed",
            "id": pmid,
            "retmode": "json",
            "cmd": "prlinks"
        }

        try:
            async with self.session.get(url, params=params, timeout=10) as response:
                response.raise_for_status()
                data = await response.json()

                links = []
                for linkset in data.get('linksets', []):
                    for idurl in linkset.get('idurllist', []):
                        for obj in idurl.get('objurls', []):
                            url_value = obj.get('url', '').strip()
                            if url_value:
                                links.append({
                                    "provider": obj.get('provider', {}).get('name', 'Unknown'),
                                    "url": url_value
                                })

                return {
                    "success": True,
                    "pmid": pmid,
                    "full_text_links": links,
                    "has_links": len(links) > 0
                }
        except Exception as e:
            return {"success": False, "error": str(e), "pmid": pmid}


# ========== å¹¶è¡Œæ‰§è¡Œå™¨ ==========

class ParallelToolExecutor:
    """å¹¶è¡Œå·¥å…·æ‰§è¡Œå™¨"""

    def __init__(self, max_workers: int = 5):
        self.max_workers = max_workers
        self.tool_chain = ToolChain()

    async def execute_parallel_async(self, tool_calls: List[Dict],
                                     async_functions: Dict[str, Callable]) -> List[Dict]:
        """å¼‚æ­¥å¹¶è¡Œæ‰§è¡Œï¼ˆç”¨äº I/O å¯†é›†å‹ä»»åŠ¡ï¼‰"""
        print(f"\nğŸ”„ å¹¶è¡Œæ‰§è¡Œ {len(tool_calls)} ä¸ªå·¥å…·...")

        tasks = []
        for call in tool_calls:
            function_name = call['function']['name']
            function_args = json.loads(call['function']['arguments'])

            if function_name in async_functions:
                task = async_functions[function_name](**function_args)
                tasks.append((call['id'], function_name, task))

        # å¹¶è¡Œæ‰§è¡Œæ‰€æœ‰ä»»åŠ¡
        results = []
        for call_id, func_name, task in tasks:
            try:
                result = await task
                results.append({
                    "tool_call_id": call_id,
                    "name": func_name,
                    "content": json.dumps(result, ensure_ascii=False)
                })
                print(f"  âœ“ {func_name} å®Œæˆ")
            except Exception as e:
                results.append({
                    "tool_call_id": call_id,
                    "name": func_name,
                    "content": json.dumps({"success": False, "error": str(e)}, ensure_ascii=False)
                })
                print(f"  âœ— {func_name} å¤±è´¥: {e}")

        return results

    def execute_parallel_threads(self, tool_calls: List[Dict],
                                 sync_functions: Dict[str, Callable]) -> List[Dict]:
        """çº¿ç¨‹å¹¶è¡Œæ‰§è¡Œï¼ˆç”¨äº CPU å¯†é›†å‹æˆ–åŒæ­¥ä»»åŠ¡ï¼‰"""
        print(f"\nğŸ”„ å¹¶è¡Œæ‰§è¡Œ {len(tool_calls)} ä¸ªå·¥å…·ï¼ˆçº¿ç¨‹æ± ï¼‰...")

        results = []
        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            future_to_call = {}

            for call in tool_calls:
                function_name = call['function']['name']
                function_args = json.loads(call['function']['arguments'])

                if function_name in sync_functions:
                    future = executor.submit(sync_functions[function_name], **function_args)
                    future_to_call[future] = (call['id'], function_name)

            # æ”¶é›†ç»“æœ
            for future in as_completed(future_to_call):
                call_id, func_name = future_to_call[future]
                try:
                    result = future.result()
                    results.append({
                        "tool_call_id": call_id,
                        "name": func_name,
                        "content": json.dumps(result, ensure_ascii=False)
                    })
                    print(f"  âœ“ {func_name} å®Œæˆ")
                except Exception as e:
                    results.append({
                        "tool_call_id": call_id,
                        "name": func_name,
                        "content": json.dumps({"success": False, "error": str(e)}, ensure_ascii=False)
                    })
                    print(f"  âœ— {func_name} å¤±è´¥: {e}")

        return results


# ========== æµå¼è¾“å‡ºç®¡ç†å™¨ ==========

class StreamingOutput:
    """æµå¼è¾“å‡ºç®¡ç†å™¨"""

    def __init__(self):
        self.buffer = []

    def write(self, text: str, delay: float = 0.02):
        """æ¨¡æ‹Ÿæ‰“å­—æ•ˆæœ"""
        for char in text:
            sys.stdout.write(char)
            sys.stdout.flush()
            time.sleep(delay)

    def write_line(self, text: str, delay: float = 0.02):
        """è¾“å‡ºä¸€è¡Œ"""
        self.write(text, delay)
        print()  # æ¢è¡Œ

    def write_section(self, title: str, content: str):
        """è¾“å‡ºç« èŠ‚"""
        print(f"\n{'='*70}")
        self.write_line(f"  {title}", 0.01)
        print(f"{'='*70}\n")
        self.write_line(content, 0.02)

    def write_progress(self, current: int, total: int, message: str = ""):
        """è¿›åº¦æ¡"""
        bar_length = 40
        progress = current / total
        filled = int(bar_length * progress)
        bar = 'â–ˆ' * filled + 'â–‘' * (bar_length - filled)

        sys.stdout.write(f'\râ³ [{bar}] {current}/{total} {message}')
        sys.stdout.flush()

        if current == total:
            print()  # å®Œæˆåæ¢è¡Œ


# ========== åŒæ­¥åŒ…è£…å‡½æ•°ï¼ˆå…¼å®¹æ—§æ¥å£ï¼‰==========

def search_pubmed(query: str, max_results: int = 10, sort: str = "relevance") -> Dict:
    """åŒæ­¥ç‰ˆæœ¬çš„æœç´¢ï¼ˆç”¨äºå‘åå…¼å®¹ï¼‰"""
    async def _search():
        async with AsyncPubMedClient() as client:
            return await client.search_pubmed(query, max_results, sort)

    return asyncio.run(_search())


def fetch_pubmed_details(pmids: List[str]) -> Dict:
    """åŒæ­¥ç‰ˆæœ¬çš„è·å–è¯¦æƒ…"""
    async def _fetch():
        async with AsyncPubMedClient() as client:
            return await client.fetch_details(pmids)

    return asyncio.run(_fetch())


def fetch_full_text_links(pmid: str) -> Dict:
    """åŒæ­¥ç‰ˆæœ¬çš„è·å–å…¨æ–‡é“¾æ¥"""
    async def _fetch():
        async with AsyncPubMedClient() as client:
            return await client.fetch_full_text_link(pmid)

    return asyncio.run(_fetch())


# ========== AI Agent å·¥å…·å®šä¹‰ï¼ˆä¼˜åŒ–ç‰ˆï¼‰==========

tools = [
    {
        "type": "function",
        "function": {
            "name": "search_pubmed",
            "description": "åœ¨ PubMed æ•°æ®åº“ä¸­æœç´¢ç§‘å­¦æ–‡çŒ®ã€‚æ”¯æŒå¹¶è¡Œæœç´¢å¤šä¸ªä¸»é¢˜ã€‚è¿”å›ç›¸å…³æ–‡ç« çš„ PMID åˆ—è¡¨ã€‚",
            "parameters": {
                "type": "object",
                "properties": {
                    "query": {
                        "type": "string",
                        "description": "æœç´¢å…³é”®è¯ï¼ˆå¦‚ï¼š'2å‹ç³–å°¿ç—…æ²»ç–—'ã€'COVID-19 ç–«è‹—æ•ˆæœ'ï¼‰"
                    },
                    "max_results": {
                        "type": "integer",
                        "description": "è¿”å›çš„æœ€å¤§ç»“æœæ•°ï¼ˆé»˜è®¤: 10ï¼‰",
                        "default": 10
                    },
                    "sort": {
                        "type": "string",
                        "description": "æ’åºæ–¹å¼ï¼š'relevance'ï¼ˆç›¸å…³æ€§ï¼‰æˆ– 'pub_date'ï¼ˆå‘è¡¨æ—¥æœŸï¼‰",
                        "default": "relevance",
                        "enum": ["relevance", "pub_date"]
                    }
                },
                "required": ["query"]
            }
        }
    },
    {
        "type": "function",
        "function": {
            "name": "fetch_pubmed_details",
            "description": "è·å–æŒ‡å®š PMID æ–‡ç« çš„è¯¦ç»†ä¿¡æ¯ï¼ˆæ ‡é¢˜ã€æ‘˜è¦ã€ä½œè€…ã€æœŸåˆŠç­‰ï¼‰ã€‚ä¼šåœ¨æœç´¢å®Œæˆåè‡ªåŠ¨è°ƒç”¨ã€‚",
            "parameters": {
                "type": "object",
                "properties": {
                    "pmids": {
                        "type": "array",
                        "items": {"type": "string"},
                        "description": "PubMed ID åˆ—è¡¨"
                    }
                },
                "required": ["pmids"]
            }
        }
    },
    {
        "type": "function",
        "function": {
            "name": "fetch_full_text_links",
            "description": "è·å–æ–‡ç« çš„å…¨æ–‡é“¾æ¥ï¼ˆå¦‚æœå¯ç”¨ï¼‰ã€‚æ”¯æŒå¹¶è¡Œè·å–å¤šç¯‡æ–‡ç« çš„é“¾æ¥ã€‚æ³¨æ„ï¼šè®¸å¤šæ–‡ç« éœ€è¦è®¢é˜…æˆ–æœºæ„è®¿é—®æƒé™ã€‚",
            "parameters": {
                "type": "object",
                "properties": {
                    "pmid": {
                        "type": "string",
                        "description": "PubMed ID"
                    }
                },
                "required": ["pmid"]
            }
        }
    }
]


# ========== ä¼˜åŒ–çš„ Agent ä¸»å¾ªç¯ï¼ˆæ”¯æŒæµå¼è¾“å‡ºï¼‰==========

async def run_agent_async(user_query: str,
                          conversation_history: List[Dict] = None,
                          enable_streaming: bool = True) -> str:
    """
    å¼‚æ­¥ AI Agentï¼ˆæ”¯æŒå¹¶è¡Œå·¥å…·è°ƒç”¨å’Œæµå¼è¾“å‡ºï¼‰

    Args:
        user_query: ç”¨æˆ·æŸ¥è¯¢
        conversation_history: å¯¹è¯å†å²
        enable_streaming: æ˜¯å¦å¯ç”¨æµå¼è¾“å‡º
    """
    streamer = StreamingOutput()
    executor = ParallelToolExecutor(max_workers=5)
    tool_chain = ToolChain()

    # åˆå§‹åŒ–æ¶ˆæ¯
    if conversation_history is None:
        messages = [
            {
                "role": "system",
                "content": """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„åŒ»å­¦æ–‡çŒ®æ£€ç´¢åŠ©æ‰‹ã€‚

**æ ¸å¿ƒèƒ½åŠ›ï¼š**
1. ç†è§£ç”¨æˆ·çš„åŒ»å­¦ç ”ç©¶éœ€æ±‚
2. æ™ºèƒ½æœç´¢ PubMed æ•°æ®åº“
3. æ”¯æŒå¹¶è¡Œæœç´¢å¤šä¸ªä¸»é¢˜ï¼ˆä¾‹å¦‚åŒæ—¶æœç´¢å¤šç§ç–¾ç—…ï¼‰
4. ç”¨æ¸…æ™°ã€ä¸“ä¸šä½†æ˜“æ‡‚çš„è¯­è¨€å‘ˆç°ç»“æœ

**å·¥å…·ä½¿ç”¨ç­–ç•¥ï¼š**
- å¦‚æœç”¨æˆ·è¦æ±‚æ¯”è¾ƒå¤šä¸ªä¸»é¢˜ï¼ŒåŒæ—¶è°ƒç”¨å¤šä¸ª search_pubmed
- å¦‚æœéœ€è¦è·å–å¤šç¯‡æ–‡ç« çš„å…¨æ–‡é“¾æ¥ï¼Œå¹¶è¡Œè°ƒç”¨ fetch_full_text_links
- ä¼˜å…ˆä½¿ç”¨å·¥å…·è·å–æœ€æ–°æ•°æ®ï¼Œè€Œä¸æ˜¯ä¾èµ–è®­ç»ƒæ•°æ®

**å›å¤é£æ ¼ï¼š**
- å…ˆç»™å‡ºç®€çŸ­æ€»ç»“
- ç”¨ç»“æ„åŒ–çš„æ–¹å¼å‘ˆç°ç»“æœ
- çªå‡ºå…³é”®å‘ç°å’Œç»Ÿè®¡æ•°æ®
- æä¾›å¯æ“ä½œçš„å»ºè®®"""
            }
        ]
    else:
        messages = conversation_history.copy()

    messages.append({"role": "user", "content": user_query})

    # æ˜¾ç¤ºæŸ¥è¯¢
    if enable_streaming:
        streamer.write_section("ğŸ” æ‚¨çš„é—®é¢˜", user_query)
    else:
        print(f"\n{'='*70}")
        print(f"ğŸ” æŸ¥è¯¢: {user_query}")
        print(f"{'='*70}\n")

    max_iterations = 5
    iteration = 0

    # å¼‚æ­¥ PubMed å®¢æˆ·ç«¯
    async with AsyncPubMedClient() as pubmed_client:
        async_functions = {
            "search_pubmed": pubmed_client.search_pubmed,
            "fetch_pubmed_details": pubmed_client.fetch_details,
            "fetch_full_text_links": pubmed_client.fetch_full_text_link
        }

        while iteration < max_iterations:
            iteration += 1

            if iteration > 1 and enable_streaming:
                streamer.write_line(f"\nâš™ï¸  å¤„ç†æ­¥éª¤ {iteration}...", 0.01)

            # è°ƒç”¨ LLM
            try:
                response = client.chat.completions.create(
                    model='deepseek-chat',
                    messages=messages,
                    tools=tools,
                    temperature=0.7
                )
            except Exception as e:
                print(f"\nâŒ AI è°ƒç”¨å¤±è´¥: {str(e)}")
                return "æŠ±æ­‰ï¼Œç³»ç»Ÿé‡åˆ°é”™è¯¯ï¼Œè¯·ç¨åé‡è¯•ã€‚"

            assistant_message = response.choices[0].message
            messages.append(assistant_message)

            # æ£€æŸ¥å·¥å…·è°ƒç”¨
            if assistant_message.tool_calls:
                tool_calls = assistant_message.tool_calls

                # åˆ¤æ–­æ˜¯å¦å¯ä»¥å¹¶è¡Œæ‰§è¡Œ
                can_parallel = tool_chain.can_execute_parallel(tool_calls)

                if can_parallel and len(tool_calls) > 1:
                    # å¹¶è¡Œæ‰§è¡Œ
                    print(f"\nğŸš€ æ£€æµ‹åˆ° {len(tool_calls)} ä¸ªç‹¬ç«‹ä»»åŠ¡ï¼Œå¯åŠ¨å¹¶è¡Œæ‰§è¡Œæ¨¡å¼")

                    # è½¬æ¢ä¸ºæ ‡å‡†æ ¼å¼
                    calls_data = []
                    for call in tool_calls:
                        calls_data.append({
                            'id': call.id,
                            'function': {
                                'name': call.function.name,
                                'arguments': call.function.arguments
                            }
                        })

                    # å¹¶è¡Œæ‰§è¡Œ
                    results = await executor.execute_parallel_async(calls_data, async_functions)

                    # æ·»åŠ æ‰€æœ‰ç»“æœåˆ°æ¶ˆæ¯å†å²
                    for result in results:
                        messages.append({
                            "role": "tool",
                            "tool_call_id": result["tool_call_id"],
                            "name": result["name"],
                            "content": result["content"]
                        })

                else:
                    # ä¸²è¡Œæ‰§è¡Œ
                    print(f"\nâ³ é¡ºåºæ‰§è¡Œ {len(tool_calls)} ä¸ªå·¥å…·...")

                    for idx, tool_call in enumerate(tool_calls, 1):
                        function_name = tool_call.function.name
                        function_args = json.loads(tool_call.function.arguments)

                        if enable_streaming:
                            streamer.write_progress(idx, len(tool_calls), f"æ‰§è¡Œ {function_name}")
                        else:
                            print(f"  [{idx}/{len(tool_calls)}] æ‰§è¡Œ {function_name}...")

                        # æ‰§è¡Œå·¥å…·
                        try:
                            if function_name in async_functions:
                                result = await async_functions[function_name](**function_args)
                            else:
                                result = {"success": False, "error": "æœªçŸ¥å·¥å…·"}

                            # æ˜¾ç¤ºç®€è¦ç»“æœ
                            if result.get('success'):
                                if function_name == "search_pubmed":
                                    print(f"    âœ“ æ‰¾åˆ° {result.get('count', 0)} ç¯‡æ–‡ç« ï¼ˆå…± {result.get('total_available', 0)} ç¯‡å¯ç”¨ï¼‰")
                                elif function_name == "fetch_pubmed_details":
                                    print(f"    âœ“ è·å– {result.get('count', 0)} ç¯‡æ–‡ç« è¯¦æƒ…")
                                elif function_name == "fetch_full_text_links":
                                    links_count = len(result.get('full_text_links', []))
                                    print(f"    âœ“ æ‰¾åˆ° {links_count} ä¸ªå…¨æ–‡é“¾æ¥")
                            else:
                                print(f"    âš ï¸  {result.get('error', 'æœªçŸ¥é”™è¯¯')}")

                            # æ·»åŠ åˆ°æ¶ˆæ¯å†å²
                            messages.append({
                                "role": "tool",
                                "tool_call_id": tool_call.id,
                                "name": function_name,
                                "content": json.dumps(result, ensure_ascii=False)
                            })

                        except Exception as e:
                            error_result = {"success": False, "error": str(e)}
                            messages.append({
                                "role": "tool",
                                "tool_call_id": tool_call.id,
                                "name": function_name,
                                "content": json.dumps(error_result, ensure_ascii=False)
                            })
                            print(f"    âœ— æ‰§è¡Œå¤±è´¥: {e}")

            else:
                # æ²¡æœ‰å·¥å…·è°ƒç”¨ï¼Œè¿”å›æœ€ç»ˆç»“æœ
                if enable_streaming:
                    streamer.write_section("ğŸ“‹ æ£€ç´¢ç»“æœ", "")
                    streamer.write_line(assistant_message.content, 0.02)
                else:
                    print(f"\n{'='*70}")
                    print("ğŸ“‹ æ£€ç´¢ç»“æœ:")
                    print(f"{'='*70}")
                    print(assistant_message.content)
                    print(f"{'='*70}\n")

                return assistant_message.content

        print("\nâš ï¸  å¤„ç†æ—¶é—´è¿‡é•¿ï¼Œè¯·ç®€åŒ–æŸ¥è¯¢åé‡è¯•")
        return "æŠ±æ­‰ï¼ŒæŸ¥è¯¢è¶…æ—¶ã€‚"


def run_agent(user_query: str,
              conversation_history: List[Dict] = None,
              enable_streaming: bool = True) -> str:
    """åŒæ­¥åŒ…è£…å™¨"""
    return asyncio.run(run_agent_async(user_query, conversation_history, enable_streaming))


# ========== å¢å¼ºçš„äº¤äº’ç•Œé¢ ==========

class EnhancedCLI:
    """å¢å¼ºçš„å‘½ä»¤è¡Œç•Œé¢"""

    def __init__(self):
        self.conversation_history = None
        self.search_history = []
        self.streamer = StreamingOutput()

    def show_welcome(self):
        """æ¬¢è¿ç•Œé¢"""
        banner = """
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                                                                      â•‘
â•‘        ğŸ¥ PubMed åŒ»å­¦æ–‡çŒ®æ™ºèƒ½æ£€ç´¢åŠ©æ‰‹ (å¢å¼ºç‰ˆ)                            â•‘
â•‘                                                                      â•‘
â•‘        âœ¨ æ–°åŠŸèƒ½:                                                     â•‘
â•‘           â€¢ å¹¶è¡Œå·¥å…·è°ƒç”¨ - åŒæ—¶æœç´¢å¤šä¸ªä¸»é¢˜                              â•‘
â•‘           â€¢ æ™ºèƒ½å·¥å…·é“¾ - è‡ªåŠ¨ä¼˜åŒ–æ‰§è¡Œé¡ºåº                                â•‘
â•‘           â€¢ æµå¼è¾“å‡º - å®æ—¶æ˜¾ç¤ºå¤„ç†è¿›åº¦                                  â•‘
â•‘                                                                      â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        """
        print(banner)

        print("\nğŸ’¡ ä½¿ç”¨æç¤º:")
        print("  â€¢ ç›´æ¥è¾“å…¥åŒ»å­¦é—®é¢˜ï¼ˆæ”¯æŒä¸­è‹±æ–‡ï¼‰")
        print("  â€¢ è¾“å…¥ 'help' æŸ¥çœ‹ç¤ºä¾‹")
        print("  â€¢ è¾“å…¥ 'history' æŸ¥çœ‹æœç´¢å†å²")
        print("  â€¢ è¾“å…¥ 'clear' å¼€å§‹æ–°å¯¹è¯")
        print("  â€¢ è¾“å…¥ 'quit' é€€å‡ºç¨‹åº")
        print("\n" + "="*70 + "\n")

    def show_examples(self):
        """ç¤ºä¾‹æŸ¥è¯¢"""
        examples = [
            ("åŸºç¡€æœç´¢", "æœç´¢å…³äºé˜¿å°”èŒ¨æµ·é»˜ç—…æœ€æ–°æ²»ç–—æ–¹æ³•çš„æ–‡çŒ®"),
            ("å¹¶è¡Œæœç´¢", "æ¯”è¾ƒç³–å°¿ç—…å’Œé«˜è¡€å‹åœ¨2024å¹´çš„ç ”ç©¶æ•°é‡"),
            ("å¤šä¸»é¢˜å¯¹æ¯”", "åŒæ—¶æœç´¢COVID-19ç–«è‹—ã€æ²»ç–—è¯ç‰©å’Œåé—ç—‡çš„ç ”ç©¶"),
            ("æ·±åº¦æ£€ç´¢", "æ‰¾5ç¯‡å…³äºCRISPRåŸºå› ç¼–è¾‘åœ¨ç™Œç—‡æ²»ç–—ä¸­çš„åº”ç”¨ï¼Œå¹¶è·å–å…¨æ–‡é“¾æ¥"),
            ("è¶‹åŠ¿åˆ†æ", "åˆ†æ2023-2024å¹´é—´äººå·¥æ™ºèƒ½åœ¨åŒ»å­¦å½±åƒè¯Šæ–­é¢†åŸŸçš„ç ”ç©¶è¶‹åŠ¿")
        ]

        print("\n" + "="*70)
        print("ğŸ“š ç¤ºä¾‹æŸ¥è¯¢ï¼ˆå±•ç¤ºå¹¶è¡Œå’Œå·¥å…·é“¾èƒ½åŠ›ï¼‰:")
        print("="*70)

        for i, (category, example) in enumerate(examples, 1):
            print(f"\n  {i}. [{category}]")
            print(f"     {example}")

        print("\n" + "="*70 + "\n")

    def show_history(self):
        """æ˜¾ç¤ºæœç´¢å†å²"""
        if not self.search_history:
            print("\nğŸ“­ æš‚æ— æœç´¢å†å²\n")
            return

        print("\n" + "="*70)
        print("ğŸ“œ æœç´¢å†å²ï¼ˆæœ€è¿‘5æ¡ï¼‰:")
        print("="*70)

        for i, item in enumerate(self.search_history[-5:], 1):
            print(f"\n  {i}. {item['query']}")
            print(f"     æ—¶é—´: {item['time']}")
            if 'summary' in item:
                print(f"     ç»“æœ: {item['summary']}")

        print("\n" + "="*70 + "\n")

    def run(self):
        """ä¸»å¾ªç¯"""
        self.show_welcome()

        while True:
            try:
                user_input = input("ğŸ” æ‚¨çš„é—®é¢˜: ").strip()

                if not user_input:
                    continue

                # å‘½ä»¤å¤„ç†
                if user_input.lower() in ['quit', 'exit', 'é€€å‡º', 'q']:
                    print("\nğŸ‘‹ æ„Ÿè°¢ä½¿ç”¨ï¼Œå†è§ï¼\n")
                    break

                if user_input.lower() in ['help', 'å¸®åŠ©', 'h']:
                    self.show_examples()
                    continue

                if user_input.lower() in ['history', 'å†å²', 'hist']:
                    self.show_history()
                    continue

                if user_input.lower() in ['clear', 'æ¸…ç©º', 'new']:
                    self.conversation_history = None
                    print("\nğŸ”„ å·²å¼€å§‹æ–°å¯¹è¯\n")
                    continue

                # æ‰§è¡ŒæŸ¥è¯¢
                start_time = time.time()
                result = run_agent(
                    user_input,
                    self.conversation_history,
                    enable_streaming=True
                )
                elapsed = time.time() - start_time

                # è®°å½•å†å²
                self.search_history.append({
                    "query": user_input,
                    "time": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                    "summary": f"è€—æ—¶ {elapsed:.1f}ç§’"
                })

                # è¯¢é—®åç»­æ“ä½œ
                print("\n" + "-"*70)
                print(f"â±ï¸  æŸ¥è¯¢è€—æ—¶: {elapsed:.1f} ç§’")
                print("-"*70)

                next_action = input("\nğŸ’¬ ç»§ç»­æé—®? (å›è½¦ç»§ç»­ / 'new' æ–°å¯¹è¯ / 'quit' é€€å‡º): ").strip()

                if next_action.lower() in ['quit', 'exit', 'q']:
                    print("\nğŸ‘‹ æ„Ÿè°¢ä½¿ç”¨ï¼Œå†è§ï¼\n")
                    break
                elif next_action.lower() in ['new', 'clear']:
                    self.conversation_history = None
                    print("\nğŸ”„ å·²å¼€å§‹æ–°å¯¹è¯\n")

            except KeyboardInterrupt:
                print("\n\nğŸ‘‹ ç¨‹åºå·²ä¸­æ–­ï¼Œå†è§ï¼\n")
                break
            except Exception as e:
                print(f"\nâŒ å‘ç”Ÿé”™è¯¯: {str(e)}\n")


# ========== æ€§èƒ½æµ‹è¯•å‡½æ•° ==========

async def benchmark_parallel_vs_serial():
    """æ€§èƒ½å¯¹æ¯”æµ‹è¯•"""
    print("\n" + "="*70)
    print("âš¡ æ€§èƒ½æµ‹è¯•ï¼šå¹¶è¡Œ vs ä¸²è¡Œ")
    print("="*70)

    queries = [
        "diabetes 2024",
        "Alzheimer disease 2024",
        "COVID-19 vaccine 2024"
    ]

    # ä¸²è¡Œæµ‹è¯•
    print("\nğŸ“Š ä¸²è¡Œæ‰§è¡Œ:")
    serial_start = time.time()
    async with AsyncPubMedClient() as client:
        for query in queries:
            result = await client.search_pubmed(query, max_results=5)
            print(f"  âœ“ {query}: {result.get('count', 0)} ç¯‡")
    serial_time = time.time() - serial_start
    print(f"  æ€»è€—æ—¶: {serial_time:.2f} ç§’")

    # å¹¶è¡Œæµ‹è¯•
    print("\nğŸš€ å¹¶è¡Œæ‰§è¡Œ:")
    parallel_start = time.time()
    async with AsyncPubMedClient() as client:
        tasks = [client.search_pubmed(q, max_results=5) for q in queries]
        results = await asyncio.gather(*tasks)
        for query, result in zip(queries, results):
            print(f"  âœ“ {query}: {result.get('count', 0)} ç¯‡")
    parallel_time = time.time() - parallel_start
    print(f"  æ€»è€—æ—¶: {parallel_time:.2f} ç§’")

    # å¯¹æ¯”
    speedup = serial_time / parallel_time
    print(f"\nâš¡ æ€§èƒ½æå‡: {speedup:.1f}x")
    print(f"   èŠ‚çœæ—¶é—´: {serial_time - parallel_time:.2f} ç§’ ({(1 - parallel_time/serial_time)*100:.1f}%)")
    print("="*70 + "\n")


# ========== ä¸»ç¨‹åº ==========

if __name__ == "__main__":
    import sys

    if len(sys.argv) > 1 and sys.argv[1] == "benchmark":
        # æ€§èƒ½æµ‹è¯•æ¨¡å¼
        asyncio.run(benchmark_parallel_vs_serial())
    else:
        # æ­£å¸¸äº¤äº’æ¨¡å¼
        cli = EnhancedCLI()
        cli.run()
